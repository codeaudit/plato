{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling neural network\n",
    "Here we will try to develop a sampling neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a Multi-Layer Perceptron with parameters.  We will use the notation of [1](http://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights), and refer to the weight from unit $i$ in layer $l-1$ to unit $j$ in layer l as $W_{ijl}$\n",
    "\n",
    "Our MLP defines some deterministic function $f(X, W)$, where $X$ is a shape $(N_{samples}, N_{dims})$ array of input data, and $W_l$ is a list of $(N_{units(l-1)}, N_{units(l)})$ weight matrix (this may later be generalized for conv-nets, etc)\n",
    "\n",
    "For 1-of-K classification, we can say that the network has the form\n",
    "\n",
    "$$p(Y|X, W) = \\prod_{n=1}^{N_{samples}}Categorical\\big(f(X_n, W)\\big)$$\n",
    "\n",
    "And restrict f(X_n, W) to output a discrete distribution over {1..K}.\n",
    "\n",
    "Applying bayes rule,\n",
    "$$ \\\\\n",
    "\\begin{align} \\\\\n",
    "p(W|X,Y)&=\\frac{p(Y|X,W)p(W)}{p(Y)} \\\\\n",
    "p(W)&\\propto p(W)p(Y|X,W) \\\\\n",
    "p(Y|X,W)&=p(W)\\prod_{n=1}^{N_{samples}}Categorical\\big(f(X_n, W)\\big)(Y_n) \\\\\n",
    "&=p(W)\\prod_{n=1}^{N_{samples}}f(X_n, W)_{Y_n} \\\\\n",
    "&\\equiv L(W|X,Y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For numerical reasons, we work in terms of log-likelihood.\n",
    "\n",
    "$$\n",
    "\\begin{align} \\\\\n",
    "logL(W|X,Y)&\\equiv log(L(W|W,Y))\\\\\n",
    "&=log(p(W))+\\sum_{n=1}^{N_{samples}}log(f(X_n,W)_{Y_n}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Lets make a few assumptions to simplify things.\n",
    "- Each weight $W_{ijl}$ has an independent prior, so $p(W)=\\prod_{ijl}p(W_{ijl})$\n",
    "- Weights are discrete - that is they can take on 1-of-K values.\n",
    "\n",
    "Now, we want to find $W$ that maximizes $p(W|X,Y)$.  We can use Gibbs sampling.  For a given weight, $\\alpha \\in (i,j,l)$, and a set of possible weights $c_k, k \\in 1...K$\n",
    "\n",
    "$$\n",
    "\\begin{align} \\\\\n",
    "p(W_{\\alpha}=c_k|W_{~\\alpha}, X, Y) &= \\frac{[L(W_{\\alpha}=c_k|W_{~\\alpha}, X, Y), k \\in 1..K]}{\\sum_k L(W_{\\alpha}=c_k|W_{~\\alpha}, X, Y)} \\\\\n",
    "p(W_{\\alpha}=c_k|W_{~\\alpha}, X, Y) &= softmax([logL(W|W_{\\alpha}=c_k, X, Y), k \\in 1...K])\\\\\n",
    "&= softmax([log(p(W))+\\sum_{n=1}^{N_{samples}}log(f(X_n,W_{w_{\\alpha}=c_k})_{Y_n}), k \\in 1...K])_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, we have an Gibbs sampling update:\n",
    "$$\n",
    "W_{\\alpha} \\sim Categorical \\Big(softmax\\big(\\big[log(p(W_{\\alpha}=c_k))+\\sum_{n=1}^{N_{samples}}log(f(X_n,W_{w_{\\alpha}=c_k})_{Y_n},k \\in 1..K\\big]\\big)\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "n_dims = 50\n",
    "hidden_sizes = [100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}