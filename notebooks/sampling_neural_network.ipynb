{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling neural network\n",
    "Here we develop a neural network that learns by Gibbs sampling the distribution over parameters.\n",
    "\n",
    "## Math\n",
    "\n",
    "We have a Multi-Layer Perceptron with parameters.  We will use the notation of [1](http://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights), and refer to the weight from unit $i$ in layer $l-1$ to unit $j$ in layer l as $W_{ijl}$\n",
    "\n",
    "Our MLP defines some deterministic function $f(X, W)$, where $X$ is a shape $(N_{samples}, N_{dims})$ array of input data, and $W_l$ is a list of $(N_{units(l-1)}, N_{units(l)})$ weight matrix (this may later be generalized for conv-nets, etc)\n",
    "\n",
    "For 1-of-K classification, we restrict f(X_n, W) to output a discrete distribution over {1..K}.  We can then say that the network defines a distibution over Y given X, W.\n",
    "\n",
    "$$p(Y|X, W) = \\prod_{n=1}^{N_{samples}}Categorical\\big(f(X_n, W)\\big)$$\n",
    "\n",
    "Now, the goal of training is to find W that maximizes $p(W|X,Y)$.  Applying bayes rule,\n",
    "\n",
    "$$ \\\\\n",
    "\\begin{align} \\\\\n",
    "p(W|X,Y)&=\\frac{p(Y|X,W)p(W)}{p(Y)} \\\\\n",
    "&\\propto p(W)p(Y|X,W) \\\\\n",
    "p(W)p(Y|X,W)&=p(W)\\prod_{n=1}^{N_{samples}}Categorical\\big(f(X_n, W)\\big)(Y_n) \\\\\n",
    "&=p(W)\\prod_{n=1}^{N_{samples}}f(X_n, W)_{Y_n} \\\\\n",
    "&\\equiv L(W|X,Y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For numerical reasons, we work in terms of log-likelihood.\n",
    "\n",
    "$$\n",
    "\\begin{align} \\\\\n",
    "logL(W|X,Y)&\\equiv log(L(W|W,Y))\\\\\n",
    "&=log(p(W))+\\sum_{n=1}^{N_{samples}}log(f(X_n,W)_{Y_n}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Lets make a few assumptions to simplify things.\n",
    "- Each weight $W_{ijl}$ has an independent prior, so $p(W)=\\prod_{ijl}p(W_{ijl})$\n",
    "- Weights are discrete - that is they can take on 1-of-K values.\n",
    "\n",
    "Now, we want to find $W$ that maximizes $p(W|X,Y)$.  We can use Gibbs sampling.  For a given weight-index, $\\alpha \\in (i,j,l)$, and a set of possible weights $c_k, k \\in 1...K$\n",
    "\n",
    "$$\n",
    "\\begin{align} \\\\\n",
    "p(W_{\\alpha}=c_k|W_{~\\alpha}, X, Y) &= \\frac{[L(W_{\\alpha}=c_k|W_{~\\alpha}, X, Y), k \\in 1..K]}{\\sum_k L(W_{\\alpha}=c_k|W_{~\\alpha}, X, Y)} \\\\\n",
    "p(W_{\\alpha}=c_k|W_{~\\alpha}, X, Y) &= softmax([logL(W|W_{\\alpha}=c_k, X, Y), k \\in 1...K])\\\\\n",
    "&= softmax([log(p(W))+\\sum_{n=1}^{N_{samples}}log(f(X_n,W_{w_{\\alpha}=c_k})_{Y_n}), k \\in 1...K])_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, we have an Gibbs sampling update:\n",
    "$$\n",
    "W_{\\alpha} \\sim Categorical \\Big(softmax\\big(\\big[log(p(W_{\\alpha}=c_k))+\\sum_{n=1}^{N_{samples}}log(f(X_n,W_{w_{\\alpha}=c_k})_{Y_n},k \\in 1..K\\big]\\big)\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "To demo this concept, we will design a simple 3-layer (input, hidden, output) network, with discrete valued weights.  We will train this on a trivial synthetic dataset consiting of clustered binary vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from general.mymath import sigm, softmax\n",
    "\n",
    "n_training_samples = 500           # Number of samples in training set\n",
    "n_test_samples = 500               # Number of samples in test set\n",
    "n_passes = 20                      # Number of Gibbs-sampling passes to make through all the weights\n",
    "n_input = 10                       # Input dimension of dataset/number of units in input layer\n",
    "n_hidden = 10                      # Number of units in hidden layer\n",
    "n_categories = 4                   # Number of categories in dataset\n",
    "possible_ws = (-1, 0, 1)           # Possible values for W\n",
    "test_every = 1                     # Test every <X> passes\n",
    "minibatch_size = None              # None for full-batch\n",
    "hidden_activation_funcion = sigm   # Nonlinear activation function for hidden layer\n",
    "seed = 3525423                     # Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "# The Synthetic Clusters Dataset consists of binary input vectors as inputs, and integer labels.  Each label\n",
    "# has an associated binary vector.  Samples are generated by taking an (input_vector, label) pair and corrupting\n",
    "# the input vector by randomly flipping some bits (see flip_noise parameter). \n",
    "from utils.datasets.synthetic_clusters import get_synthetic_clusters_dataset\n",
    "dataset = get_synthetic_clusters_dataset(n_dims = n_input, n_training=n_training_samples, n_test=n_test_samples, n_clusters = n_categories, flip_noise=0.1)\n",
    "print dataset\n",
    "x_tr, y_tr, x_ts, y_ts = dataset.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from utils.benchmarks.train_and_test import percent_argmax_correct\n",
    "\n",
    "# Define network\n",
    "b_h = np.zeros(n_hidden)\n",
    "b_o = np.zeros(n_categories)\n",
    "w_ih = np.zeros((n_input, n_hidden))\n",
    "w_ho = np.zeros((n_hidden, n_categories))\n",
    "w = [w_ih, w_ho]\n",
    "p_y_given_wx = lambda x: softmax(sigm(x.dot(w_ih)+b_h).dot(w_ho)+b_o, axis = 1)\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# Now, train the network on the dataset.\n",
    "n_samples_in_minibatch = minibatch_size if minibatch_size is not None else n_training_samples\n",
    "all_indices = [(i, j, 0) for i, j in np.ndindex(n_input, n_hidden)] + [(i, j, 1) for i, j in np.ndindex(n_hidden, n_categories)]\n",
    "\n",
    "for t in xrange(n_passes):\n",
    "    if t % test_every == 0:\n",
    "        score = percent_argmax_correct(p_y_given_wx(x_ts), y_ts)\n",
    "        print 'Pass %s of %s.  Score: %.2f%%' % (t, n_passes, score)\n",
    "    for (i, j, l) in all_indices:\n",
    "        ixs = slice(None) if minibatch_size is None else rng.choice(n_training_samples, size=minibatch_size, replace = False)\n",
    "        weight_likelihoods = np.empty((len(possible_ws), n_samples_in_minibatch))\n",
    "        old_w = w[l][i, j]\n",
    "        for k, c_k in enumerate(possible_ws):\n",
    "            w[l][i, j] = c_k\n",
    "            weight_likelihoods[k, :] = p_y_given_wx(x_tr[ixs])[np.arange(n_samples_in_minibatch), y_tr[ixs]]\n",
    "        w_ijl_dist = softmax(np.sum(np.log(weight_likelihoods), axis = 1))  # Note we ignore the prior over w for now.\n",
    "        w_ijl = rng.choice(possible_ws, p=w_ijl_dist)\n",
    "        w[l][i, j] = w_ijl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We can see that training works.  When `n_clusters` is set to 4, the expected score by guessing is 25%.  Once it's converged, our network guesses around 96% correct on average.  Our network converges within about 3 passes though the data (which says more about the simplicity of the dataset than it does about our network). \n",
    "\n",
    "Note that:\n",
    "- For simplicity we do not learn biases.  It's also not clear if biases should be discretized in the same way as weights are.\n",
    "- We have assume a uniform prior over possible weight values.  There may be some advantage to designing a prior to encourage sparse weights.\n",
    "- We could get a better score by computing a running average of p_y_given_wx(x_ts) over t, but we do not, for simplicity.\n",
    "- The code is extremely inefficient - not just because it's looping in Python, but also because it recomputes the forward pass for the entire network for each possible weight value each time we update a weight.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}